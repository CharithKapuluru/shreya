{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree & Random Forest Modeling\n",
    "- ### Background:\n",
    "The purpose of these modeling tasks is to help in figuring out (i.e. predicting) customers with the profile of having a high probability of paying back their loans. We are given the loan/lending data of a bank, between the years 2007 and 2010 for a classification task of predicting whether or not the borrower paid back their loan in full.\n",
    "\n",
    "- ### Here are what the columns represent:\n",
    "\n",
    "- credit.policy: 1 if a customer qualifies for a loan; and 0 otherwise.\n",
    "- purpose: loan purpose.\n",
    "- int.rate: the interest rate of the loan.\n",
    "- installment: monthly installment amount to be paid by the borrower.\n",
    "- log.annual.inc: the natural log of the reported annual income on file for the borrower.\n",
    "- dti: debt-to-income ratio of the borrower (borrower's total debt divided by annual income).\n",
    "- fico: the FICO credit score of the borrower at the time of loan application.\n",
    "- days.with.cr.line: the number of days the borrower has had a credit line.\n",
    "- revol.bal: borrower's revolving balance\n",
    "- revol.util: borrower's revolving line utilization rate (total credit line used relative to total credit available).\n",
    "- inq.last.6mths: number of inquiries by creditors in the last 6 months.\n",
    "- delinq.2yrs: number of times the borrower had failed to make monthly payments by the due dates in the past 2 years.\n",
    "- pub.rec: borrower's number of derogatory public records such as bankruptcy filings, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the appropriate libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the loans data file, and call it \"loans\"\n",
    "loans = pd.read_csv('loan_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we do any EDAs, lets check out the info(), head(), and describe() methods on loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view some records\n",
    "loans.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "- Let's do some data visualization, using seaborn and some built-in pandas plotting capabilities (You can also try out any other libraries you want) i.e. what really matter is the main idea behind each plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "loans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='red',\n",
    "                                              bins=30,label='Credit.Policy=1')\n",
    "loans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='green',\n",
    "                                              bins=30,label='Credit.Policy=0')\n",
    "plt.legend()\n",
    "plt.xlabel('FICO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the previous figure, based on the \"not.fully.paid\" column instead of \"credit.policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "loans[loans['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='red',\n",
    "                                              bins=30,label='not.fully.paid=1')\n",
    "loans[loans['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='green',\n",
    "                                              bins=30,label='not.fully.paid=0')\n",
    "plt.legend()\n",
    "plt.xlabel('FICO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Let's see a countplot showing loans by purpose, with the color hue defined by \"not.fully.paid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.countplot(x='purpose', hue='not.fully.paid', data=loans,palette='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize the trend between FICO score and interest rate, using jointplot from seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use seaborn's lmplot to see if the trend differed between \"not.fully.paid\" and \"credit.policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "sns.lmplot(y='int.rate', x='fico', data=loans, hue='credit.policy', col='not.fully.paid', palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the data for modeling :\n",
    "- Let's prepare our dataset for both Decision Tree and Random Forest Classification Models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick check on our data:\n",
    "loans.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that the \"purpose\" column is categorical - this means we have to transform them using dummy variables for use in sklearn. We will accomplish this step using panda's get_dummies( ) method, in such a way to take care of all other categorical columns or features on our loans dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a list of 1 element containing the string 'purpose', and name it cat_feats:\n",
    "cat_feats = ['purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we use pd.get_dummies(loans,columns=cat_feats,drop_first=True) to create a larger dataframe \n",
    "# that has new feature columns with all the dummy variables we need. We will call this dataframe as final_data:\n",
    "final_data = pd.get_dummies(loans, columns=cat_feats,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what fianl_data looks like:\n",
    "final_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split\n",
    "- Now we go ahead and split our data into training and testing datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, we use sklearn to split our data into training and testing datasets:\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_data.drop('not.fully.paid', axis=1)\n",
    "y = final_data['not.fully.paid']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Decision Tree Model:\n",
    "- We start by training a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier:\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create an instance of DecisionTreeClassifier() called dtree and then train it!\n",
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluation of our Decision Tree Model\n",
    "- Create predictions from the test set and create a classification report and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, We Train Our Random Forest Model\n",
    "- We will create an instance of the RandomForestClassifier class and fit it to our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 600)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let do some prediction with our random forest!\n",
    "rfc_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which model performed better? The Random Forest or The Decision Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It depends on what metric you are trying to optimize.\n",
    "# Notice the recall for each class for the models: Neither did very well; More feature engineering might be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
